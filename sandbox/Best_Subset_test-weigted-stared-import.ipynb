{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81575740-c4a9-488d-bac8-c4c8463dd5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b527cc7-84da-4287-9f34-fcefdc3524a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import pytest\n",
    "import sys\n",
    "import re\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7d2c35-485f-4b80-8761-2308e75b961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import best_subset as bs_pkgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2b7f862-9e01-4069-8398-a72cefeea660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['const', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
      "       'x11', 'x12', 'x13', 'x14', 'x15', 'weight'],\n",
      "      dtype='object')\n",
      "df shape: (50000, 17)\n",
      "y shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_exaustive_and_return_top(df, features, return_top=100):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    top =  df[mask] \n",
    "\n",
    "    top['rank'] = top.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "    top = top[top['rank']<=100]\n",
    "    top.drop('rank', axis=1, inplace=True)\n",
    "    top = top.reset_index(drop=True)\n",
    "    return top\n",
    "\n",
    "\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "\n",
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame):\n",
    "    \"\"\"Compares two pandas DataFrames, rounding floating-point columns to 2 decimal places.\"\"\"\n",
    "    float_cols = df1.select_dtypes(include=['float']).columns\n",
    "    df1_rounded = df1.copy()\n",
    "    df2_rounded = df2.copy()\n",
    "    df1_rounded[float_cols] = df1_rounded[float_cols].round(2)\n",
    "    df2_rounded[float_cols] = df2_rounded[float_cols].round(2)\n",
    "    pd.testing.assert_frame_equal(df1_rounded, df2_rounded, check_dtype=False)\n",
    "    print(\"Dataset Match\")\n",
    "\n",
    "def order_models_field(df):\n",
    "    df['Models'] = df['Models'].apply(\n",
    "        lambda model: \" \".join(\n",
    "            sorted(model.split(\" \"), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "        )\n",
    "    )\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "def order_models_filed_all(df):\n",
    "    df['Models'] = df['Models'].apply(lambda model: \" \".join(sorted(model.split(\" \"))) )\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_synthetic_data(seed=42, n=50000, p=15):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame X of shape (n, p+1) with columns:\n",
    "      - 'const': all ones (intercept)\n",
    "      - 'x1', 'x2', ... 'x15'\n",
    "    And a Series y with binary (0/1) outcomes drawn from a logistic model.\n",
    "    \n",
    "    Some of the 15 features have nonzero coefficients, others are zero,\n",
    "    so there's meaningful signal to detect in a logistic regression.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Generate random features ~ N(0,1)\n",
    "    X_base = np.random.randn(n, p)\n",
    "    \n",
    "    # 2) Define \"true\" coefficients\n",
    "    #    For instance, let's say 5 features matter:\n",
    "    #    x1, x2, x3, x4, x5 have some nonzero betas.\n",
    "    #    The remaining x6..x15 have 0 effect.\n",
    "    betas_true = np.array([1.5, -2.0, 0.75, 1.25, -0.5] + [0]*(p-5))\n",
    "    #     -> length = 15\n",
    "    \n",
    "    # 3) Linear predictor: X_base dot betas_true\n",
    "    #    shape -> (n, )\n",
    "    lin_pred = X_base.dot(betas_true)\n",
    "    \n",
    "    # 4) Convert linear predictor to probability via logistic function\n",
    "    #    p_i = 1 / (1 + exp(-lin_pred))\n",
    "    prob = 1.0 / (1.0 + np.exp(-lin_pred))\n",
    "    \n",
    "    # 5) Draw binary outcomes y from Bernoulli(prob)\n",
    "    y_vals = np.random.binomial(1, prob)\n",
    "    \n",
    "    # 6) Create a DataFrame for features, plus an intercept column\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i+1}\" for i in range(p)])\n",
    "    df.insert(0, \"const\", 1.0)  # intercept\n",
    "    \n",
    "    # 7) Create a Series for y\n",
    "    y = pd.Series(y_vals, name=\"y\")\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "\n",
    "def create_synthetic_data_logistic(seed=42, n=50000, p=15):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame X of shape (n, p+2) with columns:\n",
    "      - 'const': all ones (intercept)\n",
    "      - 'x1', 'x2', ... 'x15'\n",
    "      - 'weight': randomly generated weights between 0 and 100\n",
    "    And a Series y with binary (0/1) outcomes drawn from a logistic model.\n",
    "    \n",
    "    Some of the 15 features have nonzero coefficients, others are zero,\n",
    "    so there's meaningful signal to detect in a logistic regression.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Generate random features ~ N(0,1)\n",
    "    X_base = np.random.randn(n, p)\n",
    "    \n",
    "    # 2) Define \"true\" coefficients\n",
    "    #    For instance, let's say 5 features matter:\n",
    "    #    x1, x2, x3, x4, x5 have some nonzero betas.\n",
    "    #    The remaining x6..x15 have 0 effect.\n",
    "    betas_true = np.array([1.5, -2.0, 0.75, 1.25, -0.5] + [0]*(p-5))\n",
    "    #     -> length = 15\n",
    "    \n",
    "    # 3) Linear predictor: X_base dot betas_true\n",
    "    #    shape -> (n, )\n",
    "    lin_pred = X_base.dot(betas_true)\n",
    "    \n",
    "    # 4) Convert linear predictor to probability via logistic function\n",
    "    #    p_i = 1 / (1 + exp(-lin_pred))\n",
    "    prob = 1.0 / (1.0 + np.exp(-lin_pred))\n",
    "    \n",
    "    # 5) Draw binary outcomes y from Bernoulli(prob)\n",
    "    y_vals = np.random.binomial(1, prob)\n",
    "    \n",
    "    # 6) Create a DataFrame for features, plus an intercept column\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i+1}\" for i in range(p)])\n",
    "    df.insert(0, \"const\", 1.0)  # intercept\n",
    "\n",
    "    # 7) Generate weights between 0 and 100\n",
    "    weights = np.random.uniform(0, 100, size=n)\n",
    "    df['weight'] = weights  # Add 'weight' column\n",
    "    \n",
    "    # 8) Create a Series for y\n",
    "    y = pd.Series(y_vals, name=\"y\")\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "\n",
    "def create_synthetic_data_linear_regression(seed=42, n=50000, p=15, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Creates synthetic data for linear regression.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed for reproducibility.\n",
    "        n: Number of samples.\n",
    "        p: Number of features (excluding the intercept).\n",
    "        sigma: Standard deviation of the error term.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A DataFrame `df` containing the features (including 'const' and 'weight') \n",
    "               and a Series `y` representing the target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Generate random features ~ N(0,1)\n",
    "    X_base = np.random.randn(n, p)\n",
    "\n",
    "    # 2) Define \"true\" coefficients (including the intercept)\n",
    "    #    Let's say 5 features have a non-zero effect.\n",
    "    betas_true = np.array([2.0, 1.5, -2.0, 0.75, 1.25, -0.5] + [0] * (p - 5))\n",
    "    # betas_true now includes the intercept (e.g., 2.0) in the first position.\n",
    "\n",
    "    # 3) Generate weights between 0 and 100\n",
    "    weights = np.random.uniform(0, 100, size=n)\n",
    "\n",
    "    # 4) Create a DataFrame for features, plus an intercept column\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i + 1}\" for i in range(p)])\n",
    "    df.insert(0, \"const\", 1.0)  # intercept\n",
    "\n",
    "    # 5) Linear predictor: X dot betas_true (including intercept)\n",
    "    #    shape -> (n, )\n",
    "    lin_pred = df.drop(columns=['const']).dot(betas_true[1:]) + betas_true[0] # Account for intercept in betas_true\n",
    "\n",
    "    # 6) Generate the target variable y with added noise:\n",
    "    #    y = linear predictor + error\n",
    "    #    where error ~ N(0, sigma^2)\n",
    "    y_vals = lin_pred + np.random.normal(0, sigma, size=n)\n",
    "\n",
    "    # 7) Add 'weight' column to DataFrame\n",
    "    df['weight'] = weights\n",
    "\n",
    "    # 8) Create a Series for y\n",
    "    y = pd.Series(y_vals, name=\"y\")\n",
    "\n",
    "    return df, y\n",
    "\n",
    "\n",
    "def create_synthetic_data_ordinal_logit(seed=42, n_samples=50000, n_features=15, n_classes=3, \n",
    "                                      beta_scale=1.0, class_separation=1.0):\n",
    "    \"\"\"\n",
    "    Creates synthetic data for ordinal logistic regression.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed for reproducibility\n",
    "        n_samples: Number of observations\n",
    "        n_features: Number of features (excluding intercept)\n",
    "        n_classes: Number of ordinal classes (3-5 recommended)\n",
    "        beta_scale: Scale factor for coefficient magnitudes\n",
    "        class_separation: Controls spread between cutpoints\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (df, y) where df contains features + weights, y contains ordinal labels\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 1. Generate features matrix with intercept\n",
    "    X_base = np.random.randn(n_samples, n_features)\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i+1}\" for i in range(n_features)])\n",
    "    df.insert(0, \"const\", 1.0)\n",
    "\n",
    "    # 2. Generate true parameters\n",
    "    n_cutpoints = n_classes - 1\n",
    "    \n",
    "    # Coefficients (first 5 features have non-zero effects)\n",
    "    beta_true = np.zeros(n_features + 1)  # +1 for intercept\n",
    "    beta_true[0] = 1.0  # Intercept\n",
    "    beta_true[1:6] = np.array([1.5, -2.0, 0.75, 1.25, -0.5]) * beta_scale\n",
    "    \n",
    "    # Cutpoints (sorted for identifiability)\n",
    "    theta_true = np.sort(np.random.randn(n_cutpoints) * class_separation)\n",
    "\n",
    "    # 3. Compute linear predictor\n",
    "    X_mat = df.values\n",
    "    XB = X_mat @ beta_true  # Shape (n_samples,)\n",
    "\n",
    "    # 4. Calculate class probabilities using proportional odds model\n",
    "    z = theta_true[:, None] - XB  # Shape (n_cutpoints, n_samples)\n",
    "    cumulative_probs = 1 / (1 + np.exp(-z))  # CDF values\n",
    "    \n",
    "    # Pad with 0 (left) and 1 (right) for class probabilities\n",
    "    padded_probs = np.vstack([np.zeros((1, n_samples)),\n",
    "                             cumulative_probs,\n",
    "                             np.ones((1, n_samples))])\n",
    "    \n",
    "    # Calculate class probabilities via differences\n",
    "    class_probs = np.diff(padded_probs, axis=0)  # Shape (n_classes, n_samples)\n",
    "    class_probs = class_probs.T  # Shape (n_samples, n_classes)\n",
    "\n",
    "    # 5. Generate ordinal labels\n",
    "    u = np.random.rand(n_samples)\n",
    "    cumulative_probs = np.cumsum(class_probs, axis=1)\n",
    "    y = (u[:, None] < cumulative_probs).argmax(axis=1)\n",
    "\n",
    "    # 6. Add weights and return\n",
    "    df[\"weight\"] = np.random.uniform(0, 100, size=n_samples)\n",
    "    return df, pd.Series(y, name=\"y\")\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df, y = create_synthetic_data_logistc(p=15)\n",
    "    # print(df.head())\n",
    "    # print(y.head())\n",
    "    print(df.columns)\n",
    "    print(\"df shape:\", df.shape)\n",
    "    print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef3cfa-0fcb-4bd6-b7be-89b73531032d",
   "metadata": {},
   "source": [
    "# Compoare two exhaustive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5d2a9aa3-955e-45f5-bb44-2c1cc6619e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "results = bs_pkgs.best_subset_exhaustive_logistic(df, y, candidates)\n",
    "# g.shape, Is.shape, len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d3863308-b0fa-4116-a924-b4f6dfc913bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'fico', 'fico_lt', 'gt_fico', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15']\n",
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "print(candidates)\n",
    "results2 = bs_pkgs.best_subset_exhaustive(df, y, candidates, method='logistic')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c9557e1e-8c89-42f3-bca1-e04df96dc641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(results,results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fa3f2-ffb0-442d-8fb9-b0298ac79639",
   "metadata": {},
   "source": [
    "# Test 1\n",
    "\n",
    "* Compare Exaustive results without `weights` using forced features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85d5bc-9e30-435d-90ea-0fd40e1a542a",
   "metadata": {},
   "source": [
    "## Run Exhaustive Model. Use it as Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d3c433b4-ea53-4d59-8aa0-cb784ea05516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "# candidates = df.columns[1:-1].tolist()\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, candidates, method='logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a21af9-1921-45c4-9776-1e4b96ecef60",
   "metadata": {},
   "source": [
    "# Filter Exhaustive\n",
    "\n",
    "* filter models with these features `['x1','x2', '*fico*']`. Must have x1, x2 and at least 1 fico feature (for example)\n",
    "* Return top 100 models given the conditions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4031d0dd-a6c9-4bd1-b274-a36504f59495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\2124947351.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Models'] = df['Models'].apply(lambda model: \" \".join(sorted(model.split(\" \"))) )\n",
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\405614052.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2', '*fico*'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)\n",
    "exhaustive_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "18053ee9-bf4c-43be-a022-64bd535e1054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'fico', 'fico_lt', 'gt_fico', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15']\n",
      "Finished Var Family: 2  Skipped\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "100\n",
      "29\n",
      "CPU times: total: 1.05 s\n",
      "Wall time: 255 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df, y = create_synthetic_data_weights(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "cands = df.columns[1:].tolist()\n",
    "cands.remove('weight')\n",
    "print(cands)\n",
    "res_weights, _ , _ = bs_pkgs.best_subset_bb_logistic_with_priority_import(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2', \"*fico*\"], method='logistic'  )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)\n",
    " \n",
    "res_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e8041-b4fe-47f5-80f6-904216719b17",
   "metadata": {},
   "source": [
    "##  Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7d5c9810-ea5d-4019-ae1d-e4fa0c9f1c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b548039-1dc9-4d90-b5ab-18747e787f27",
   "metadata": {},
   "source": [
    "# Test 2\n",
    "\n",
    "* Repeat step above with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3ecb11b8-21c9-4429-89db-583f28403d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\2124947351.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Models'] = df['Models'].apply(lambda model: \" \".join(sorted(model.split(\" \"))) )\n",
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\2129568915.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "# candidates = df.columns[1:-1].tolist()\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, candidates, weights=np.array(df['weight']), method='logistic')\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2', '*fico*'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)\n",
    "exhaustive_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f26ba951-7f79-4c90-892e-5eb0cdccd0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'fico', 'fico_lt', 'gt_fico', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15']\n",
      "Finished Var Family: 2  Skipped\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "100\n",
      "29\n",
      "CPU times: total: 984 ms\n",
      "Wall time: 279 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df, y = create_synthetic_data_weights(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "cands = df.columns[1:].tolist()\n",
    "cands.remove('weight')\n",
    "print(cands)\n",
    "res_weights, _ , _ = bs_pkgs.best_subset_bb_logistic_with_priority_import(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2', \"*fico*\"], weights=df['weight'], method='logistic'  )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)\n",
    " \n",
    "res_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6da5fbf7-b6b1-45e9-995d-26b7fa7198e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc9628-dd67-42b7-a93a-4131026abea4",
   "metadata": {},
   "source": [
    "# Test 3: Oridinal - No Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "54e62b4c-437d-49a6-8fdc-9e845804403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15']\n",
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\2124947351.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Models'] = df['Models'].apply(lambda model: \" \".join(sorted(model.split(\" \"))) )\n",
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\391318099.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_ordinal_logit(n_features=15, n_classes=3)\n",
    "cands = df.columns.tolist()\n",
    "cands.remove('const')\n",
    "cands.remove('weight')\n",
    "print(cands)\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, cands,  method='ordinal')\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "84b0e4b2-dcd8-4c0a-a0e7-bbff5c678ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15']\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "100\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_ordinal_logit(n_features=15, n_classes=3)\n",
    "cands = df.columns.tolist()\n",
    "cands.remove('const')\n",
    "cands.remove('weight')\n",
    "# print(cands)\n",
    "res_weights, _ , _= bs_pkgs.best_subset_bb_logistic_with_priority_import(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2'], method='ordinal' )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5e172-6cdd-42b4-a4f6-0f8669a906f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4201b-2dac-4bb5-921f-3c9a5d506fab",
   "metadata": {},
   "source": [
    "# Test 4: OLS - No Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "04a67820-976c-46b4-a73a-8d611e687766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['const']\n",
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\2124947351.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Models'] = df['Models'].apply(lambda model: \" \".join(sorted(model.split(\" \"))) )\n",
      "C:\\Users\\enisb\\AppData\\Local\\Temp\\ipykernel_19508\\4068210159.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(984, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, y = create_synthetic_data_linear_regression(p=15)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "# candidates = df.columns[1:-1].tolist()\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, candidates, method='ols')\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)\n",
    "exhaustive_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f4c9a877-1a58-49ae-a389-df1386cf71fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "100\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\enisb\\python\\best-subset\\best_subset\\best_subset_bfs_weigthed_stared_import.py:509: UserWarning: This is experimental and not recommended for production use until further testing is completed\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_linear_regression(p=15)\n",
    "cands = df.columns.tolist()\n",
    "cands.remove('const')\n",
    "cands.remove('weight')\n",
    " \n",
    "res_weights, _ , _= bs_pkgs.best_subset_bb_logistic_with_priority_import(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2'], method='ols' )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0cf2f8-6182-43e2-86be-7c58389097a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c009fe63-4646-4448-87f4-dc0ade608d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f064aa-689a-49fc-a889-b9fc3ba61ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3d429833-ff1f-4ae8-9d2a-72a3811559fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(845526.7225987429)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loglike(params, X, y, W):\n",
    "    q = 2 * y - 1\n",
    "    return np.sum(W * np.log(cdf(q * np.dot(X, params))))\n",
    "\n",
    "def cdf(X):\n",
    "    X = np.asarray(X)\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def I(params, X, W):\n",
    "    X = np.array(X)\n",
    " \n",
    "    L = cdf(np.dot(X, params))\n",
    "    return -np.dot(W * L * (1 - L) * X.T, X)\n",
    "\n",
    "def score(params, X, y, W):    \n",
    "    L = cdf(np.dot(X, params))\n",
    "    return np.dot(W * (y - L), X)\n",
    "\n",
    "cands = ['const', 'x1', 'x2']\n",
    "df[cands]\n",
    "X = np.asarray(df[cands])\n",
    "weights = np.asarray(df['weight'])\n",
    "nobs = df.shape[0]\n",
    "W = (weights / np.sum(weights)) * nobs # normalized weights\n",
    "W = weights\n",
    "avg = np.sum(W * y) / np.sum(W)\n",
    "null_model = np.log(avg / (1 - avg))\n",
    "theta_0 = np.append(null_model, np.zeros(X.shape[1] - 1))\n",
    "g = score(theta_0, X, y, W)\n",
    "Is = I(theta_0, X, W)\n",
    "-g.T.dot(np.linalg.inv(Is)).dot(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a4488bb-5111-4f99-9aab-c63ca3be941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(df['weight'].values, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c12ea72f-9fdb-4c41-b6db-7a53f671cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "CPU times: total: 2min 50s\n",
      "Wall time: 42.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var Number</th>\n",
       "      <th>Models</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>x2 x1</td>\n",
       "      <td>17140.282747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>x2 x4</td>\n",
       "      <td>15300.586378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2</td>\n",
       "      <td>x2 x3</td>\n",
       "      <td>12622.364252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2</td>\n",
       "      <td>x2 x5</td>\n",
       "      <td>11698.923275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2</td>\n",
       "      <td>x2 x48</td>\n",
       "      <td>11036.236241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>5</td>\n",
       "      <td>x2 x1 x4 x18 x15</td>\n",
       "      <td>21418.674221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>5</td>\n",
       "      <td>x2 x1 x4 x15 x45</td>\n",
       "      <td>21418.662492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>5</td>\n",
       "      <td>x2 x1 x4 x13 x48</td>\n",
       "      <td>21418.614501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>5</td>\n",
       "      <td>x2 x1 x4 x15 x37</td>\n",
       "      <td>21418.472611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>5</td>\n",
       "      <td>x2 x1 x4 x13 x26</td>\n",
       "      <td>21418.281903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Var Number            Models        Scores\n",
       "99            2             x2 x1  17140.282747\n",
       "98            2             x2 x4  15300.586378\n",
       "97            2             x2 x3  12622.364252\n",
       "96            2             x2 x5  11698.923275\n",
       "95            2            x2 x48  11036.236241\n",
       "..          ...               ...           ...\n",
       "304           5  x2 x1 x4 x18 x15  21418.674221\n",
       "303           5  x2 x1 x4 x15 x45  21418.662492\n",
       "302           5  x2 x1 x4 x13 x48  21418.614501\n",
       "301           5  x2 x1 x4 x15 x37  21418.472611\n",
       "300           5  x2 x1 x4 x13 x26  21418.281903\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "a_5, b_5, c_5= bs6.best_subset_bb_logistic_with_priority(df, y, 100, start=2, stop=5,  method=\"dfs\", candidates=df.columns[1:].tolist()) # , forced_vars=['x1', 'x2'])\n",
    "a_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "47e64910-cbf6-4430-a0ab-5ee1c7fd1644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Result using np.linalg.inv: 395.2539871247952\n",
      "Result using scipy.linalg.cho_solve: 395.2539871187649\n",
      "Result using np.linalg.solve: 395.2539871247954\n",
      "\n",
      "Timing:\n",
      "Time using np.linalg.inv: 0.019066 seconds\n",
      "Time using scipy.linalg.cho_solve: 0.011001 seconds\n",
      "Time using np.linalg.solve: 0.003004 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import time\n",
    "\n",
    "# Generate a random symmetric positive-definite matrix I and a random vector v\n",
    "np.random.seed(42)\n",
    "n = 500  # Matrix size\n",
    "I = np.random.rand(n, n)\n",
    "I = np.dot(I, I.T)  # Make it symmetric positive-definite\n",
    "v = np.random.rand(n)\n",
    "\n",
    "# Method 1: Using np.linalg.inv\n",
    "start_inv = time.time()\n",
    "I_inv = np.linalg.inv(I)\n",
    "result_inv = v.T @ I_inv @ v\n",
    "end_inv = time.time()\n",
    "\n",
    "# Method 2: Using scipy.linalg.cho_solve\n",
    "start_cho = time.time()\n",
    "L = scipy.linalg.cho_factor(I)  # Perform Cholesky factorization\n",
    "result_cho = v.T @ scipy.linalg.cho_solve(L, v)  # Solve using the factorization\n",
    "end_cho = time.time()\n",
    "\n",
    "# Method 3: Using np.linalg.solve\n",
    "start_solve = time.time()\n",
    "result_solve = v.T @ np.linalg.solve(I, v)  # Directly solve the system\n",
    "end_solve = time.time()\n",
    "\n",
    "# Print results\n",
    "print(\"Results:\")\n",
    "print(f\"Result using np.linalg.inv: {result_inv}\")\n",
    "print(f\"Result using scipy.linalg.cho_solve: {result_cho}\")\n",
    "print(f\"Result using np.linalg.solve: {result_solve}\")\n",
    "\n",
    "print(\"\\nTiming:\")\n",
    "print(f\"Time using np.linalg.inv: {end_inv - start_inv:.6f} seconds\")\n",
    "print(f\"Time using scipy.linalg.cho_solve: {end_cho - start_cho:.6f} seconds\")\n",
    "print(f\"Time using np.linalg.solve: {end_solve - start_solve:.6f} seconds\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f395b83b-cbdc-4569-86f7-65ed320e3234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# Create a heap\n",
    "heap = []\n",
    "heapq.heappush(heap, 10)\n",
    "heapq.heappush(heap, 5)\n",
    "heapq.heappush(heap, 20)\n",
    "type(heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3b420cc2-1d3b-446a-bfd5-26255a4a138f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, key, branches, n, forced_vars=None):\n",
    "        \"\"\"\n",
    "        key: the current subset of variables (excluding 'const')\n",
    "        branches: how many branches remain\n",
    "        n: the target subset size or node parameter\n",
    "        forced_vars: list of variables that must stay in every subset\n",
    "        \"\"\"\n",
    "        if forced_vars is None:\n",
    "            forced_vars = []\n",
    "\n",
    "        self.key = key                # full subset (list of strings)\n",
    "        self.key2 = key[:n]           # partial subset for bounding\n",
    "        self.branch_id = n - branches + 1\n",
    "        self.n = n\n",
    "        self.forced_vars = forced_vars\n",
    "\n",
    "        self.child = []\n",
    "        self.key_list = []\n",
    "        self.has_branches = branches\n",
    "\n",
    "    def add_children(self):\n",
    "        \"\"\"\n",
    "        Create child nodes by popping one feature at a time\n",
    "        but skip if that would drop any forced_var from the subset.\n",
    "        \"\"\"\n",
    "        visit = self.has_branches - 1\n",
    "\n",
    "        for has_branches_new, _ in enumerate(range(visit, 0, -1)):\n",
    "            child_branch_id = self.n - has_branches_new - 1\n",
    "            temp = self.key[:]\n",
    "\n",
    "            # Sanity check: child_branch_id might be out of range\n",
    "            if child_branch_id < 0 or child_branch_id >= len(temp):\n",
    "                continue\n",
    "\n",
    "            removed_feat = temp.pop(child_branch_id)\n",
    "            # If removing that feature leads to losing forced var, skip\n",
    "            if removed_feat in self.forced_vars:\n",
    "                continue\n",
    "\n",
    "            # Also skip if after removal, any forced var is not in temp\n",
    "            if not all(fv in temp for fv in self.forced_vars):\n",
    "                continue\n",
    "\n",
    "            # If we haven't pruned, then child is valid\n",
    "            # We also skip if it doesn't actually reduce the subset size\n",
    "            if len(temp) == self.n - 1:\n",
    "                # This line in the original code was used to skip \n",
    "                # same-size sets, but let's keep it for consistency:\n",
    "                continue\n",
    "\n",
    "            new_node = Node(\n",
    "                temp, \n",
    "                has_branches_new + 2, \n",
    "                self.n, \n",
    "                forced_vars=self.forced_vars\n",
    "            )\n",
    "            self.child.append(new_node)\n",
    "            self.key_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5e7bcccf-22fa-403e-9aff-eebeea7566c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 2\n",
    "n = var \n",
    "root = Node(['x1', 'x2'] , branches=n + 1, n=n, forced_vars=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fba1f920-e34a-41b7-9afc-009e891be66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq = []\n",
    "heapq.heappush(pq, (-250, root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6b51be83-9480-48a0-970d-fc69d3b7864a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-250, <__main__.Node at 0x1a544ce6b60>)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23f8db-ed88-40b4-9b81-dbed6916eba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightGBM",
   "language": "python",
   "name": "lightgbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
