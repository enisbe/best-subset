{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81575740-c4a9-488d-bac8-c4c8463dd5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b527cc7-84da-4287-9f34-fcefdc3524a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import pytest\n",
    "import sys\n",
    "import re\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7d2c35-485f-4b80-8761-2308e75b961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import best_subset as bs_pkgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b7f862-9e01-4069-8398-a72cefeea660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['const', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
      "       'x11', 'x12', 'x13', 'x14', 'x15', 'weight'],\n",
      "      dtype='object')\n",
      "df shape: (50000, 17)\n",
      "y shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_exaustive_and_return_top(df, features, return_top=100):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    top =  df[mask] \n",
    "\n",
    "    top['rank'] = top.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "    top = top[top['rank']<=100]\n",
    "    top.drop('rank', axis=1, inplace=True)\n",
    "    top = top.reset_index(drop=True)\n",
    "    return top\n",
    "\n",
    "\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "\n",
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame):\n",
    "    \"\"\"Compares two pandas DataFrames, rounding floating-point columns to 2 decimal places.\"\"\"\n",
    "    float_cols = df1.select_dtypes(include=['float']).columns\n",
    "    df1_rounded = df1.copy()\n",
    "    df2_rounded = df2.copy()\n",
    "    df1_rounded[float_cols] = df1_rounded[float_cols].round(2)\n",
    "    df2_rounded[float_cols] = df2_rounded[float_cols].round(2)\n",
    "    pd.testing.assert_frame_equal(df1_rounded, df2_rounded, check_dtype=False)\n",
    "    print(\"Dataset Match\")\n",
    "\n",
    "def order_models_field(df):\n",
    "    df['Models'] = df['Models'].apply(\n",
    "        lambda model: \" \".join(\n",
    "            sorted(model.split(\" \"), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "        )\n",
    "    )\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "def order_models_filed_all(df):\n",
    "    df['Models'] = df['Models'].apply(lambda model: \" \".join(sorted(model.split(\" \"))) )\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_synthetic_data(seed=42, n=50000, p=15):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame X of shape (n, p+1) with columns:\n",
    "      - 'const': all ones (intercept)\n",
    "      - 'x1', 'x2', ... 'x15'\n",
    "    And a Series y with binary (0/1) outcomes drawn from a logistic model.\n",
    "    \n",
    "    Some of the 15 features have nonzero coefficients, others are zero,\n",
    "    so there's meaningful signal to detect in a logistic regression.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Generate random features ~ N(0,1)\n",
    "    X_base = np.random.randn(n, p)\n",
    "    \n",
    "    # 2) Define \"true\" coefficients\n",
    "    #    For instance, let's say 5 features matter:\n",
    "    #    x1, x2, x3, x4, x5 have some nonzero betas.\n",
    "    #    The remaining x6..x15 have 0 effect.\n",
    "    betas_true = np.array([1.5, -2.0, 0.75, 1.25, -0.5] + [0]*(p-5))\n",
    "    #     -> length = 15\n",
    "    \n",
    "    # 3) Linear predictor: X_base dot betas_true\n",
    "    #    shape -> (n, )\n",
    "    lin_pred = X_base.dot(betas_true)\n",
    "    \n",
    "    # 4) Convert linear predictor to probability via logistic function\n",
    "    #    p_i = 1 / (1 + exp(-lin_pred))\n",
    "    prob = 1.0 / (1.0 + np.exp(-lin_pred))\n",
    "    \n",
    "    # 5) Draw binary outcomes y from Bernoulli(prob)\n",
    "    y_vals = np.random.binomial(1, prob)\n",
    "    \n",
    "    # 6) Create a DataFrame for features, plus an intercept column\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i+1}\" for i in range(p)])\n",
    "    df.insert(0, \"const\", 1.0)  # intercept\n",
    "    \n",
    "    # 7) Create a Series for y\n",
    "    y = pd.Series(y_vals, name=\"y\")\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "\n",
    "def create_synthetic_data_logistic(seed=42, n=50000, p=15):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame X of shape (n, p+2) with columns:\n",
    "      - 'const': all ones (intercept)\n",
    "      - 'x1', 'x2', ... 'x15'\n",
    "      - 'weight': randomly generated weights between 0 and 100\n",
    "    And a Series y with binary (0/1) outcomes drawn from a logistic model.\n",
    "    \n",
    "    Some of the 15 features have nonzero coefficients, others are zero,\n",
    "    so there's meaningful signal to detect in a logistic regression.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Generate random features ~ N(0,1)\n",
    "    X_base = np.random.randn(n, p)\n",
    "    \n",
    "    # 2) Define \"true\" coefficients\n",
    "    #    For instance, let's say 5 features matter:\n",
    "    #    x1, x2, x3, x4, x5 have some nonzero betas.\n",
    "    #    The remaining x6..x15 have 0 effect.\n",
    "    betas_true = np.array([1.5, -2.0, 0.75, 1.25, -0.5] + [0]*(p-5))\n",
    "    #     -> length = 15\n",
    "    \n",
    "    # 3) Linear predictor: X_base dot betas_true\n",
    "    #    shape -> (n, )\n",
    "    lin_pred = X_base.dot(betas_true)\n",
    "    \n",
    "    # 4) Convert linear predictor to probability via logistic function\n",
    "    #    p_i = 1 / (1 + exp(-lin_pred))\n",
    "    prob = 1.0 / (1.0 + np.exp(-lin_pred))\n",
    "    \n",
    "    # 5) Draw binary outcomes y from Bernoulli(prob)\n",
    "    y_vals = np.random.binomial(1, prob)\n",
    "    \n",
    "    # 6) Create a DataFrame for features, plus an intercept column\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i+1}\" for i in range(p)])\n",
    "    df.insert(0, \"const\", 1.0)  # intercept\n",
    "\n",
    "    # 7) Generate weights between 0 and 100\n",
    "    weights = np.random.uniform(0, 100, size=n)\n",
    "    df['weight'] = weights  # Add 'weight' column\n",
    "    \n",
    "    # 8) Create a Series for y\n",
    "    y = pd.Series(y_vals, name=\"y\")\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "\n",
    "def create_synthetic_data_linear_regression(seed=42, n=50000, p=15, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Creates synthetic data for linear regression.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed for reproducibility.\n",
    "        n: Number of samples.\n",
    "        p: Number of features (excluding the intercept).\n",
    "        sigma: Standard deviation of the error term.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A DataFrame `df` containing the features (including 'const' and 'weight') \n",
    "               and a Series `y` representing the target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Generate random features ~ N(0,1)\n",
    "    X_base = np.random.randn(n, p)\n",
    "\n",
    "    # 2) Define \"true\" coefficients (including the intercept)\n",
    "    #    Let's say 5 features have a non-zero effect.\n",
    "    betas_true = np.array([2.0, 1.5, -2.0, 0.75, 1.25, -0.5] + [0] * (p - 5))\n",
    "    # betas_true now includes the intercept (e.g., 2.0) in the first position.\n",
    "\n",
    "    # 3) Generate weights between 0 and 100\n",
    "    weights = np.random.uniform(0, 100, size=n)\n",
    "\n",
    "    # 4) Create a DataFrame for features, plus an intercept column\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i + 1}\" for i in range(p)])\n",
    "    df.insert(0, \"const\", 1.0)  # intercept\n",
    "\n",
    "    # 5) Linear predictor: X dot betas_true (including intercept)\n",
    "    #    shape -> (n, )\n",
    "    lin_pred = df.drop(columns=['const']).dot(betas_true[1:]) + betas_true[0] # Account for intercept in betas_true\n",
    "\n",
    "    # 6) Generate the target variable y with added noise:\n",
    "    #    y = linear predictor + error\n",
    "    #    where error ~ N(0, sigma^2)\n",
    "    y_vals = lin_pred + np.random.normal(0, sigma, size=n)\n",
    "\n",
    "    # 7) Add 'weight' column to DataFrame\n",
    "    df['weight'] = weights\n",
    "\n",
    "    # 8) Create a Series for y\n",
    "    y = pd.Series(y_vals, name=\"y\")\n",
    "\n",
    "    return df, y\n",
    "\n",
    "\n",
    "def create_synthetic_data_ordinal_logit(seed=42, n_samples=50000, n_features=15, n_classes=3, \n",
    "                                      beta_scale=1.0, class_separation=1.0):\n",
    "    \"\"\"\n",
    "    Creates synthetic data for ordinal logistic regression.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed for reproducibility\n",
    "        n_samples: Number of observations\n",
    "        n_features: Number of features (excluding intercept)\n",
    "        n_classes: Number of ordinal classes (3-5 recommended)\n",
    "        beta_scale: Scale factor for coefficient magnitudes\n",
    "        class_separation: Controls spread between cutpoints\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (df, y) where df contains features + weights, y contains ordinal labels\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 1. Generate features matrix with intercept\n",
    "    X_base = np.random.randn(n_samples, n_features)\n",
    "    df = pd.DataFrame(X_base, columns=[f\"x{i+1}\" for i in range(n_features)])\n",
    "    df.insert(0, \"const\", 1.0)\n",
    "\n",
    "    # 2. Generate true parameters\n",
    "    n_cutpoints = n_classes - 1\n",
    "    \n",
    "    # Coefficients (first 5 features have non-zero effects)\n",
    "    beta_true = np.zeros(n_features + 1)  # +1 for intercept\n",
    "    beta_true[0] = 1.0  # Intercept\n",
    "    beta_true[1:6] = np.array([1.5, -2.0, 0.75, 1.25, -0.5]) * beta_scale\n",
    "    \n",
    "    # Cutpoints (sorted for identifiability)\n",
    "    theta_true = np.sort(np.random.randn(n_cutpoints) * class_separation)\n",
    "\n",
    "    # 3. Compute linear predictor\n",
    "    X_mat = df.values\n",
    "    XB = X_mat @ beta_true  # Shape (n_samples,)\n",
    "\n",
    "    # 4. Calculate class probabilities using proportional odds model\n",
    "    z = theta_true[:, None] - XB  # Shape (n_cutpoints, n_samples)\n",
    "    cumulative_probs = 1 / (1 + np.exp(-z))  # CDF values\n",
    "    \n",
    "    # Pad with 0 (left) and 1 (right) for class probabilities\n",
    "    padded_probs = np.vstack([np.zeros((1, n_samples)),\n",
    "                             cumulative_probs,\n",
    "                             np.ones((1, n_samples))])\n",
    "    \n",
    "    # Calculate class probabilities via differences\n",
    "    class_probs = np.diff(padded_probs, axis=0)  # Shape (n_classes, n_samples)\n",
    "    class_probs = class_probs.T  # Shape (n_samples, n_classes)\n",
    "\n",
    "    # 5. Generate ordinal labels\n",
    "    u = np.random.rand(n_samples)\n",
    "    cumulative_probs = np.cumsum(class_probs, axis=1)\n",
    "    y = (u[:, None] < cumulative_probs).argmax(axis=1)\n",
    "\n",
    "    # 6. Add weights and return\n",
    "    df[\"weight\"] = np.random.uniform(0, 100, size=n_samples)\n",
    "    return df, pd.Series(y, name=\"y\")\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df, y = create_synthetic_data_logistic(p=15)\n",
    "    # print(df.head())\n",
    "    # print(y.head())\n",
    "    print(df.columns)\n",
    "    print(\"df shape:\", df.shape)\n",
    "    print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fa3f2-ffb0-442d-8fb9-b0298ac79639",
   "metadata": {},
   "source": [
    "# Test 1\n",
    "\n",
    "* Compare Exaustive results without `weights` using forced features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85d5bc-9e30-435d-90ea-0fd40e1a542a",
   "metadata": {},
   "source": [
    "## Run Exhaustive Model. Use it as Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c433b4-ea53-4d59-8aa0-cb784ea05516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "# candidates = df.columns[1:-1].tolist()\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, candidates, method='logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a21af9-1921-45c4-9776-1e4b96ecef60",
   "metadata": {},
   "source": [
    "# Filter Exhaustive\n",
    "\n",
    "* filter models with these features `['x1','x2', '*fico*']`. Must have x1, x2 and at least 1 fico feature (for example)\n",
    "* Return top 100 models given the conditions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4031d0dd-a6c9-4bd1-b274-a36504f59495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2', '*fico*'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)\n",
    "exhaustive_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18053ee9-bf4c-43be-a022-64bd535e1054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 2  Skipped\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "CPU times: total: 641 ms\n",
      "Wall time: 173 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "cands = df.columns[1:].tolist()\n",
    "cands.remove('weight')\n",
    " \n",
    "res_weights, _ , _ = bs_pkgs.best_subset(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2', \"*fico*\"], method='logistic')\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)\n",
    " \n",
    "res_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e8041-b4fe-47f5-80f6-904216719b17",
   "metadata": {},
   "source": [
    "##  Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5c9810-ea5d-4019-ae1d-e4fa0c9f1c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b548039-1dc9-4d90-b5ab-18747e787f27",
   "metadata": {},
   "source": [
    "# Test 2\n",
    "\n",
    "* Repeat step above with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ecb11b8-21c9-4429-89db-583f28403d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "# candidates = df.columns[1:-1].tolist()\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, candidates, weights=np.array(df['weight']), method='logistic')\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2', '*fico*'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)\n",
    "exhaustive_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f26ba951-7f79-4c90-892e-5eb0cdccd0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 2  Skipped\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 188 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "cands = df.columns[1:].tolist()\n",
    "cands.remove('weight')\n",
    " \n",
    "res_weights, _ , _ = bs_pkgs.best_subset(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2', \"*fico*\"], weights=df['weight'], method='logistic'  )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)\n",
    " \n",
    "res_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f80cf365",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'forced_exact'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[1;32mc:\\users\\enisb\\python\\best-subset\\best_subset\\best_subset_bfs.py:364\u001b[0m, in \u001b[0;36mbest_subset_bb_logistic_with_priority\u001b[1;34m(df, y, nbest, start, stop, candidates, forced_vars, weights, normalize, method)\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Var Family:\u001b[39m\u001b[38;5;124m\"\u001b[39m, n, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Skipped\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m models, scores, count \u001b[38;5;241m=\u001b[39m \u001b[43mtraverse_tree_best_first\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidates_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidates_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordered_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforced_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconst\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m varn \u001b[38;5;241m=\u001b[39m [n] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores)\n\u001b[0;32m    374\u001b[0m all_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count\n",
      "File \u001b[1;32mc:\\users\\enisb\\python\\best-subset\\best_subset\\best_subset_bfs.py:309\u001b[0m, in \u001b[0;36mtraverse_tree_best_first\u001b[1;34m(X, y, n, nbest, candidates_map, candidates, g, Is, forced_vars, e, const)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_1 \u001b[38;5;241m<\u001b[39m bound[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m \u001b[43mcur_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m cur_node\u001b[38;5;241m.\u001b[39mchild:\n\u001b[0;32m    311\u001b[0m     child_score_2 \u001b[38;5;241m=\u001b[39m get_score(child\u001b[38;5;241m.\u001b[39mkey2)\n",
      "File \u001b[1;32mc:\\users\\enisb\\python\\best-subset\\best_subset\\best_subset_bfs.py:229\u001b[0m, in \u001b[0;36mNode.add_children\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    227\u001b[0m removed_feat \u001b[38;5;241m=\u001b[39m temp\u001b[38;5;241m.\u001b[39mpop(child_branch_id)\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(fv \u001b[38;5;129;01min\u001b[39;00m temp \u001b[38;5;28;01mfor\u001b[39;00m fv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforced_exact\u001b[49m):\n\u001b[0;32m    230\u001b[0m     Node\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Node' object has no attribute 'forced_exact'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df, y = create_synthetic_data_logistic(p=15)\n",
    "df.rename(columns ={ \"x3\": 'fico', \"x4\": 'fico_lt', \"x5\": \"fico_gt\",  \"x5\": \"gt_fico\"}, inplace=True)\n",
    "cands = df.columns[1:].tolist()\n",
    "cands.remove('weight')\n",
    " \n",
    "res_weights, _ , _ = bs_pkgs.best_subset(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=[], weights=df['weight'], method='logistic'  )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)\n",
    " \n",
    "res_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6da5fbf7-b6b1-45e9-995d-26b7fa7198e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc9628-dd67-42b7-a93a-4131026abea4",
   "metadata": {},
   "source": [
    "# Test 3: Oridinal - No Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54e62b4c-437d-49a6-8fdc-9e845804403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15']\n",
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_ordinal_logit(n_features=15, n_classes=3)\n",
    "cands = df.columns.tolist()\n",
    "cands.remove('const')\n",
    "cands.remove('weight')\n",
    "print(cands)\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, cands,  method='ordinal')\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84b0e4b2-dcd8-4c0a-a0e7-bbff5c678ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_ordinal_logit(n_features=15, n_classes=3)\n",
    "cands = df.columns.tolist()\n",
    "cands.remove('const')\n",
    "cands.remove('weight')\n",
    "# print(cands)\n",
    "res_weights, _ , _= bs_pkgs.best_subset (df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2'], method='ordinal' )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39c5e172-6cdd-42b4-a4f6-0f8669a906f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4201b-2dac-4bb5-921f-3c9a5d506fab",
   "metadata": {},
   "source": [
    "# Test 4: OLS - No Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04a67820-976c-46b4-a73a-8d611e687766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 1\n",
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n",
      "Total Models: 32767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(984, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, y = create_synthetic_data_linear_regression(p=15)\n",
    "candidates = df.columns.tolist()\n",
    "candidates.remove('const')\n",
    "candidates.remove('weight')\n",
    "# candidates = df.columns[1:-1].tolist()\n",
    "results =  bs_pkgs.best_subset_exhaustive(df, y, candidates, method='ols')\n",
    "def check_if_features_in(df, features):\n",
    "    mask = df['Models'].notna()  # Ensure we exclude NaN values    \n",
    "    for feature in features:\n",
    "        if \"*\" in feature:\n",
    "            feature = feature.replace(\"*\", \"\")\n",
    "            mask &= df['Models'].str.contains(f'{feature}', case=False)\n",
    "        else:        \n",
    "            mask &= df['Models'].str.contains(rf'\\b{feature}\\b', regex=True, na=False)\n",
    "    return df[mask]\n",
    "# print(a_bfs_v2.shape, check_if_features_in(a_bfs_v2, features).shape)\n",
    "exhaustive_filter_ = check_if_features_in(results, ['x1','x2'])\n",
    "exhaustive_filter_ = order_models_filed_all(exhaustive_filter_)\n",
    "exhaustive_filter_['rank'] = exhaustive_filter_.groupby(\"Var Number\")['Scores'].rank(ascending=False)\n",
    "exhaustive_filter_ = exhaustive_filter_[exhaustive_filter_['rank']<=100]\n",
    "exhaustive_filter_.drop('rank', axis=1, inplace=True)\n",
    "exhaustive_filter = exhaustive_filter_.reset_index(drop=True)\n",
    "exhaustive_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4c9a877-1a58-49ae-a389-df1386cf71fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Var Family: 2\n",
      "Finished Var Family: 3\n",
      "Finished Var Family: 4\n",
      "Finished Var Family: 5\n",
      "Finished Var Family: 6\n",
      "Finished Var Family: 7\n",
      "Finished Var Family: 8\n",
      "Finished Var Family: 9\n",
      "Finished Var Family: 10\n",
      "Finished Var Family: 11\n",
      "Finished Var Family: 12\n",
      "Finished Var Family: 13\n",
      "Finished Var Family: 14\n",
      "Finished Var Family: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\enisb\\python\\best-subset\\best_subset\\best_subset_bfs.py:447: UserWarning: OLS method is experimental and not recommended for production use until further testing is completed\n",
      "  warnings.warn(\"OLS method is experimental and not recommended for production use until further testing is completed\")\n"
     ]
    }
   ],
   "source": [
    "df, y = create_synthetic_data_linear_regression(p=15)\n",
    "cands = df.columns.tolist()\n",
    "cands.remove('const')\n",
    "cands.remove('weight')\n",
    " \n",
    "res_weights, _ , _= bs_pkgs.best_subset(df, y, 100, start=2, stop=15,  candidates=cands,  forced_vars=['x1', 'x2'], method='ols' )\n",
    "res_weights = order_models_filed_all(res_weights)\n",
    "res_weights = res_weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c009fe63-4646-4448-87f4-dc0ade608d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Match\n"
     ]
    }
   ],
   "source": [
    "compare_dataframes(exhaustive_filter,res_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightgbm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
